{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset with Deep Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST MLP with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.11.0  Device: cuda\n"
     ]
    }
   ],
   "source": [
    "''' 2. 딥러닝 모델을 설계할 때 활용하는 장비 확인 '''\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 3. MNIST 데이터 다운로드 (Train set, Test set 분리하기) '''\n",
    "train_dataset = datasets.MNIST(root = \"./data/MNIST\",\n",
    "                               train = True,\n",
    "                               download = True,\n",
    "                               transform = transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.MNIST(root = \"./data/MNIST\",\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "''' 4. 데이터 확인하기 (1) '''\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' 5. 데이터 확인하기 (2) '''\n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10 * pltsize, pltsize))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
    "    plt.title('Class: ' + str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. Multi Layer Perceptron (MLP) 모델 설계하기 '''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5 # 몇 퍼센트의 노드에 대해 가중치를 계산하지 않을 것인지 명시\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        # 시그모이드 함수의 결괏값에 대해 Dropout을 적용, x 값에 적용하며 p 값은 몇 퍼센트의 노드에 대해 계산하지 않을 것인지 조정\n",
    "        # training 옵션을 통해 학습 상태일 때와 검증 상태일 때, Dropout을 다르게 적용 → 학습 과정 속에서는 랜덤으로 노드를 선택해 가중치 업데이트 X\n",
    "        # 평가 과정 속에서는 모든 노드를 이용해 Output을 계산 → modle.train()을 명시할 때 True, model.eval()으로 명시할 때 False\n",
    "        # 이론상 일반화 성능이 높아지는 것은 학습 데이터셋과 검증 데이터셋의 피처 및 레이블의 분포 간 많은 차이가 있을 때 발생\n",
    "        # 많은 차이가 없을 경우 성능이 떨어질 수도 있고, 이 경우 Epoch 수를 늘려주는 것이 효과적이며 보통 ReLU()와 잘 어울림\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 8. MLP 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 9. 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucky\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 0.720936\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.767642\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.701451\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.337036\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.621025\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.390235\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.660209\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.365473\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.358118\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.703325\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.4202, \tTest Accuracy: 87.40 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.423455\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.224402\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.494899\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.584355\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.561509\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.732986\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.397021\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.717928\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.546369\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.434500\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.4023, \tTest Accuracy: 88.01 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.247481\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.412809\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.364452\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.722453\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.495307\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.570370\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.322814\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.429705\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.466981\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.504792\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.3881, \tTest Accuracy: 88.43 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.555132\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.482650\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.281832\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.642116\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.408216\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.769800\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.423791\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.611238\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.237573\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.448270\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.3732, \tTest Accuracy: 88.84 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.290625\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.405400\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.547765\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.265923\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.567989\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.668051\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.445047\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.276574\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.698490\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.588167\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.3627, \tTest Accuracy: 89.11 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.387878\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.233689\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.461769\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.712501\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.220225\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.215089\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.415664\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.182084\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.254851\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.309665\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.3512, \tTest Accuracy: 89.32 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.244073\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.420922\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.426249\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.225112\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.393379\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.479669\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.355966\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.383539\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.411679\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.382157\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.3449, \tTest Accuracy: 89.65 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.419361\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.428635\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.568847\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.563787\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.518234\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.404737\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.248285\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.512071\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.425579\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.616281\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.3357, \tTest Accuracy: 89.89 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.256513\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.391187\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.270154\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.382614\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.403849\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.326784\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.496247\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.358064\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.342941\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.159603\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.3287, \tTest Accuracy: 90.04 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.433972\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.163218\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.646429\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.241068\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.271538\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.328403\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.244108\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.586612\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.137370\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.306764\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.3205, \tTest Accuracy: 90.42 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' 10. MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST MLP With Dropout RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 6. Multi Layer Perceptron (MLP) 모델 설계하기 '''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x\n",
    "        # F.sigmoid() 를 F.relu()로 바꿔주기만 하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.320282\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 2.099095\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 1.282736\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.817711\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.777772\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.334972\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.313171\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.703080\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.516463\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.740639\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.3248, \tTest Accuracy: 90.81 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.427911\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.412913\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.379662\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.196036\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.839702\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.241880\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.299476\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.311229\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.637215\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.285391\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.2229, \tTest Accuracy: 93.54 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.358066\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.136892\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.283399\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.484883\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.233250\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.293460\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.148916\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.329164\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.153688\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.213629\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.1750, \tTest Accuracy: 94.84 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.484446\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.175355\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.274377\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.060351\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.498231\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.099256\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.208226\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.381701\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.132238\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.155596\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.1424, \tTest Accuracy: 95.69 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.217964\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.104833\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.081969\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.232939\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.155607\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.527849\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.138276\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.140240\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.085651\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.086931\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.1243, \tTest Accuracy: 96.25 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.178271\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.083246\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.122542\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.096146\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.206298\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.090613\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.395710\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.135922\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.140199\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.153653\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.1125, \tTest Accuracy: 96.55 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.019756\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.104263\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.064047\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.116344\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.154217\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.031487\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.113744\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.078074\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.140093\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.457554\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.1027, \tTest Accuracy: 96.79 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.116541\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.125116\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.264407\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.078539\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.297060\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.143618\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.177862\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.030972\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.339792\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.030010\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0956, \tTest Accuracy: 97.00 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.129371\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.040939\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.212164\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.216952\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.203538\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.150425\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.109306\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.230166\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.096709\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.078999\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0876, \tTest Accuracy: 97.26 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.061485\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.097408\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.030292\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.080642\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.204576\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.063926\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.164788\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.045984\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.079669\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.127797\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0844, \tTest Accuracy: 97.39 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST MLP with Dropout ReLU BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Normalization은 다양한 차원에 따라 적용되는 함수명이 다름\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        # MLP 내의 각 Layer에서 데이터는 1-Dimension 크기의 벡터 값을 계산하기 때문에 nn.BatchNorm1d 를 사용\n",
    "        # Activation 함수 적용 이전과 이후 시점에 대한 여부는 연구자마다 의견이 다름\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512) # 첫 번째 FCL의 Output이 512 크기의 벡터값이기 때문에 512차원으로 설정\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256) # 두 번째 FCL의 Output이 256 크기의 벡터이기 떄문에 256 차원으로 설정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x) # 첫 번째 FCL의 Output을 위에서 정의한 'self.batch_norm1'의 Input으로 이용\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x) # 위와 동일\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.494573\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.537417\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.524715\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.214445\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.239259\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.254848\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.121371\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.326364\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.286771\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.263716\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.1552, \tTest Accuracy: 95.27 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.072243\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.180157\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.198783\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.088101\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.182816\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.266445\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.168766\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.617953\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.286819\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.380705\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.1163, \tTest Accuracy: 96.43 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.187091\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.111449\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.242230\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.472855\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.250158\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.164034\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.216687\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.139680\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.044198\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.201364\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0961, \tTest Accuracy: 97.10 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.446227\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.242441\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.229398\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.080810\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.019484\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.216497\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.281809\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.299519\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.220614\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.270114\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0888, \tTest Accuracy: 97.32 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.229377\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.535183\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.092234\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.243664\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.039456\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.074845\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.331196\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.132005\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.153581\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.117052\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0800, \tTest Accuracy: 97.54 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.101646\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.149519\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.241591\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.266987\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.091970\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.361068\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.132134\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.448234\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.152541\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.030892\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0758, \tTest Accuracy: 97.70 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.243533\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.096475\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.155683\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.121371\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.206736\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.186331\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.077610\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.117916\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.248987\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.080878\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0720, \tTest Accuracy: 97.78 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.039306\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.089371\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.098441\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.093112\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.026887\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.030700\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.205212\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.073652\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.141481\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.316498\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0715, \tTest Accuracy: 97.85 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.283817\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.184343\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.076929\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.055537\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.076698\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.354329\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.086346\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.240813\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.031423\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.591768\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0649, \tTest Accuracy: 98.04 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.054409\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.028742\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.202191\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.098732\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.255448\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.157499\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.412436\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.070068\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.261305\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.199118\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0656, \tTest Accuracy: 97.89 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST MLP With Dropout ReLU BN He"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.init as init # Weight, Bias 등 딥러닝 모델에서 초깃값으로 설정되는 요소에 대한 모듈 \n",
    "def weight_init(m): # MLP 모델 내의 Weight를 초기화할 부분을 설정하기 위해 함수 정의\n",
    "    if isinstance(m, nn.Linear): # MLP 모델을 구성하고 있는 파라미터 중 nn.Linear에 해당하는 파라미터 값에 대해서만 지정\n",
    "        init.kaiming_uniform_(m.weight.data) # nn.Linear에 해당하는 파라미터 값에 대해 he_initialization을 이용해 파라미터 값 초기화\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "model.apply(weight_init) # 위에서 정의한 함수를 Net() 클래스의 인스턴스인 model에 적용\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.590274\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.717140\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.565695\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.541182\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.513088\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.496360\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.438316\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.339089\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.541938\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.332615\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.2205, \tTest Accuracy: 93.43 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.650676\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.166724\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.218639\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.385776\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.373290\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.133800\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.246293\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.453357\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.178696\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.570097\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.1784, \tTest Accuracy: 94.49 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.081700\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.119393\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.152342\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.342411\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.306707\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.294446\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.185128\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.319765\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.175454\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.363606\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.1548, \tTest Accuracy: 95.30 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.320576\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.359987\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.275405\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.304927\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.274289\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.237825\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.083400\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.419341\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.233243\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.166062\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.1332, \tTest Accuracy: 95.93 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.223326\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.172536\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.216348\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.221879\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.281522\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.147003\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.057684\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.054957\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.196338\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.296074\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.1214, \tTest Accuracy: 96.38 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.201985\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.274633\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.136252\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.130276\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.655408\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.242369\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.190014\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.117326\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.218643\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.156623\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.1082, \tTest Accuracy: 96.66 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.054909\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.141255\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.044751\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.170718\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.089906\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.153272\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.121383\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.146713\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.564248\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.100381\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.1021, \tTest Accuracy: 96.83 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.195713\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.208994\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.504919\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.130141\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.442547\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.592393\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.312580\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.305670\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.462582\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.177313\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0971, \tTest Accuracy: 96.99 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.194596\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.257386\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.345430\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.404438\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.047404\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.044707\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.034186\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.130601\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.381624\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.249408\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0934, \tTest Accuracy: 97.12 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.061872\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.363209\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.129860\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.094691\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.055570\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.257614\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.047839\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.069340\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.093113\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.086099\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0867, \tTest Accuracy: 97.42 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST MLP With Dropout ReLU BN He Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' 7. Optimizer, Objective Function 설정하기 '''\n",
    "import torch.nn.init as init\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_uniform_(m.weight.data)\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "model.apply(weight_init)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 3.347716\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.083784\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.536624\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.186708\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.489060\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.555615\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.504929\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.554814\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.280780\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.514773\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.1266, \tTest Accuracy: 96.02 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.229228\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.051498\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.176703\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.126580\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.118078\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.145144\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.342205\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.262520\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.251091\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.198709\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.1140, \tTest Accuracy: 96.22 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.137631\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.203787\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.284247\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.374242\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.248339\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.327500\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.175881\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.180630\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.305102\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.100369\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0984, \tTest Accuracy: 97.17 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.065575\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.135198\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.346308\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.145294\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.244121\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.223478\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.031585\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.076597\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.043008\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.142485\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0893, \tTest Accuracy: 97.15 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.742529\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.142013\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.044971\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.135023\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.339618\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.129696\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.121946\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.189454\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.180354\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.072446\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0733, \tTest Accuracy: 97.85 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.064388\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.192187\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.070817\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.157324\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.068478\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.093060\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.180349\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.153817\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.031022\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.244736\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0780, \tTest Accuracy: 97.59 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.233209\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.150875\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.208616\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.024745\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.157026\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.111830\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.130996\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.212915\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.296106\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.114633\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0707, \tTest Accuracy: 97.86 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.181945\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.442874\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.409466\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.161905\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.273125\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.131643\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.128148\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.059913\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.160920\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.160960\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0739, \tTest Accuracy: 97.82 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.099559\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.013865\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.059386\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.092767\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.121127\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.079640\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.278020\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.133999\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.284996\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.175755\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0672, \tTest Accuracy: 98.00 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.347492\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.023204\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.294758\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.009278\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.054885\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.316807\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.013054\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.310209\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.017630\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.220503\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0648, \tTest Accuracy: 98.05 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68756eb6c044f31c46e3e1f38723aea1f0146198488dd3d60c0e4241eb6f7dd0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
